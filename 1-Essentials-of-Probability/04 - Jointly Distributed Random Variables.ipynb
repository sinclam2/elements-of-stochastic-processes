{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Random Variables\n",
    "\n",
    "\n",
    "### Motivating Example\n",
    "\n",
    "- Let's consider an experiment where we flip a coin twice\n",
    "    - Let $X$ represent the outcome of the first flip\n",
    "    - Let $Y$ represent the outcome of the second flip\n",
    "    \n",
    "##### Are $X$ and $Y$ random variables?\n",
    "- **Recall**: for $X$ and $Y$ to be random variables, we require the following:\n",
    "    1. $X$ and $Y$ must only be able to take on a finite number of possible values\n",
    "        - This condition is satisfied since both $X$ and $Y$ can only take on two values (H or T)\n",
    "    2. $X$ and $Y$ must be numerical outcomes of a random phenomenon\n",
    "        - This condition is also satisfied, since a coin flip is definitely a random phenomenon\n",
    "\n",
    "- So **we've confirmed that $X$ and $Y$ are random variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the probability space for $X$ and $Y$?\n",
    "- **Recall**: a probability space requires three things\n",
    "    1. A **sample space** $\\Omega$ which is the set of all possible outcomes\n",
    "        - We already know there are only two possible outcomes so $\\Omega = \\{H,T\\}$\n",
    "    2. A **set of events** $F$ where each even is a set containing zero or more outcomes\n",
    "        - The set of possible events is the set of all possible combinations of these events\n",
    "            - This means $F = \\{H, T, \\text{Both }H\\text{ and }T,\\text{Neither }H\\text{ nor }T\\}$\n",
    "    3. The **assignment of probabilities** to the events\n",
    "        - We can figure out the probabilities for all the events in $F$ pretty easily\n",
    "            - $P(H)=1/2$\n",
    "            - $P(T)=1/2$\n",
    "            - $P(\\text{Both }H\\text{ and }T) = 0$ since we can't get both\n",
    "            - $P(\\text{Neither }H\\text{ nor }T) = 0$ since we have to get one of the two\n",
    "                \n",
    "- Therefore, **these two random variables are defined on the same probability space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are the outcomes of the two events related?\n",
    "\n",
    "- For example, if we get a heads on the first flip, what should we expect the second flip to be?\n",
    "    - Well, the gambler's fallacy tells us that just because we got a heads doesn't mean we're more likely to get a tails on the next flip\n",
    "- **In other words, if we know the value of $X$, we're not any better at predicting the value of $Y$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can write this relationship more mathematically as:\n",
    "\n",
    "$$\n",
    "P(Y=y | X=x) = P(Y=y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This equation is saying the same thing that we said above: the probability of the second flip being $y$ is not affected by the first flip being $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Independent Random Variables\n",
    "\n",
    "Two random variables $X$ and $Y$ defined on a common probability space $\\Omega$ are independent if, for any numbers $x$ and $y$ we have:\n",
    "\n",
    "$$\n",
    "P(X \\leq x \\cap Y \\leq y) = P(X \\leq x) \\cdot P(Y \\leq y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalized (and more confusing) version of this is for random variables $X_{1},X_{2},...,X_{n}$:\n",
    "\n",
    "$$\n",
    "P(X_{1}\\leq x_{1} \\cap X_{2}\\leq x_{2}\\cap ... \\cap X_{n}\\leq x_{n}) = \\prod_{i=1}^{n}P(X_{i}\\leq x_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Joint Distribution Function\n",
    "\n",
    "- If we look at the left side of the equation above, we can think of it as a function that takes in $n$ values, and returns a number between 0 and 1\n",
    "    - This is exactly what the **joint distribution function is**:\n",
    "    \n",
    "$$\n",
    "F(x_{1},x_{2},...,x_{n}) = P(X_{1}\\leq x_{1} \\cap X_{2}\\leq x_{2}\\cap ... \\cap X_{n}\\leq x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note**: the values are **cumulative** i.e. its the probability that each $X_{i}$ is **less than or equal to** $x_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Marginal Distribution Function\n",
    "\n",
    "- If we want to think about probabilities for a single random variable at a time, we use the **marginal distribution function**:\n",
    "\n",
    "$$\n",
    "F_{X_{i}}(x_{i}) = P(X_{i}\\leq x_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Ideas About Independence and Distribution Functions\n",
    "\n",
    "- We can see that it's easy to swap out the terms in our original definition for independence with the distribution functions:\n",
    "\n",
    "$$\n",
    "F(x_{1},x_{2},...,x_{n}) = F(x_{1})\\cdot F(x_{2})...F(x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Joint Mass Function\n",
    "\n",
    "- If we have **discrete** random variables $X_{1},X_{2},...,X_{n}$, then we define the **joint mass function** as:\n",
    "\n",
    "$$\n",
    "p(x_{1},x_{2},...,x_{n}) = P(X_{1}=x_{1} \\cap X_{2}=x_{2} \\cap...\\cap X_{n}=x_{n})\n",
    "$$\n",
    "\n",
    "- **Recall**: if $X_{i}$ was continuous instead of discrete, what is the value of $P(X_{1}=x_{1} \\cap X_{2}=x_{2} \\cap...\\cap X_{n}=x_{n})$\n",
    "    - **It's zero**\n",
    "        - Recall that a continuous random variable doesn't really ever equal a single value, but instead lies within a range\n",
    "            - E.g. What is the probability that it will rain 1cm tonight?\n",
    "                - ZERO\n",
    "                    - This is because the probability that 1cm falls (not 0.9999999 cm or 1.00000001) is negligible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, for our discrete random variables, they are independent if:\n",
    "\n",
    "$$\n",
    "p(x_{1},x_{2},...,x_{n}) = p_{X_{1}}(x_{1})\\cdot p_{X_{2}}(x_{2})\\cdot ... \\cdot p_{X_{n}}(x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Joint Density Function\n",
    "\n",
    "- Let's return to $F(x_{1},x_{2},...,x_{n}) = P(X_{1}\\leq x_{1} \\cap X_{2}\\leq x_{2}\\cap ... \\cap X_{n}\\leq x_{n})$ and we'll let each $X_{i}$ be a **continuous random variable**\n",
    "\n",
    "- If the random variables are all independent, then we have already shown $F(x_{1},x_{2},...,x_{n}) = F(x_{1})\\cdot F(x_{2})...F(x_{n})$\n",
    "\n",
    "- So now, what if we want to take the derivative of our **joint distribution function** with respect to one of the random variables, say $X_{1}$\n",
    "    - This means:\n",
    "    \n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{1}}F(x_{1},x_{2},...,x_{n}) = \\frac{\\partial}{\\partial x_{1}} \\left (F(x_{1})\\cdot F(x_{2})...F(x_{n}) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We recall from our first year calculus that if we're taking the partial derivative of the product of a bunch of functions where only one is a function of the variable, we can treat all the other functions as constants, i.e.:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}f(x)g(y)h(z) = g(y)h(z)\\frac{d}{d x}f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, this means that for our expression above:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{1}} \\left (F(x_{1})\\cdot F(x_{2})...F(x_{n}) \\right ) = \\frac{d}{dx_{1}}F_{X_{1}}(x_{1})\\left (  F(x_{2})...F(x_{n}) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall from earlier that the probability density function of a continuous random variable is defined as:\n",
    "\n",
    "$$\n",
    "f_{X}(x) = \\frac{d}{dx}F_{X}(x) = \\frac{d}{dx}\\left (P(X \\leq x) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plugging this into our partial derivative above:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{1}} \\left (F(x_{1})\\cdot F(x_{2})...F(x_{n}) \\right ) = f_{X_{1}}(x_{1})\\left (  F(x_{2})...F(x_{n}) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We could repeat this process for each random variable to give:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^{n}}{\\partial x_{1}\\partial x_{2}...\\partial x_{n}}F(x_{1},x_{2},...,x_{n}) = f_{X_{1}}(x_{1})\\cdot f_{X_{2}}(x_{2})\\cdot f_{X_{n}}(x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The expression $\\frac{\\partial^{n}}{\\partial x_{1}\\partial x_{2}...\\partial x_{n}}F(x_{1},x_{2},...,x_{n})$ is called the **joint density function** of the random variables and is denoted $f(x_{1},x_{2},...,x_{n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, as we derived above, if the random variables are all independent:\n",
    "\n",
    "$$\n",
    "f(x_{1},x_{2},...,x_{n}) = f_{X_{1}}(x_{1})\\cdot f_{X_{2}}(x_{2})\\cdot f_{X_{n}}(x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, if we swap out the $f$ for a $p$, this is basically the same as the discrete version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem 4.3\n",
    "\n",
    "If $X$ and $Y$ are independent and have finite second moments, they are uncorrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof of Theorem 4.3\n",
    "\n",
    "*Definitions*\n",
    "\n",
    "- Independence\n",
    "\n",
    "$$\n",
    "P(X \\leq x \\cap Y \\leq y) = P(X \\leq x) \\cdot P(Y \\leq y)\n",
    "$$\n",
    "\n",
    "- Moments\n",
    "\n",
    "$$\n",
    "n^{th}\\text{ Moment of the Mean} = E[X^{n}] \n",
    "$$\n",
    "\n",
    "- Uncorrelated\n",
    "\n",
    "$$\n",
    "Cov(X,Y) = 0\n",
    "$$\n",
    "\n",
    "- Covariance\n",
    "\n",
    "$$\n",
    "Cov(X,Y) = \\sigma_{XY} = E[XY] - E[X]E[Y]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, if $X$ and $Y$ are uncorrelated:\n",
    "\n",
    "$$\n",
    "E[XY] = E[X]E[Y]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll do the discrete case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E[X] = \\sum_{x}x\\cdot P(X=x); E[Y] = \\sum_{y}y\\cdot P(Y=y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\implies E[X]E[Y] = \\left (\\sum_{x}x\\cdot P(X=x) \\right ) \\cdot \\left (  \\sum_{y}y\\cdot P(Y=y) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\sum_{x}\\sum_{y}xy\\cdot P(X=x)P(Y=y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since $X$ and $Y$ are independent, we know $ P(X=x)P(Y=y) = P(X=x \\cap Y=y)$\n",
    "\n",
    "$$\n",
    "\\implies \\sum_{x}\\sum_{y}xy\\cdot P(X=x)P(Y=y) = \\sum_{x}\\sum_{y}xy\\cdot P(X=x \\cap Y=y) = E[XY]\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
