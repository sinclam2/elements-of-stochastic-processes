{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Random Variables\n",
    "\n",
    "\n",
    "### Motivating Example\n",
    "\n",
    "- Let's consider an experiment where we flip a coin twice\n",
    "    - Let $X$ represent the outcome of the first flip\n",
    "    - Let $Y$ represent the outcome of the second flip\n",
    "    \n",
    "##### Are $X$ and $Y$ random variables?\n",
    "- **Recall**: for $X$ and $Y$ to be random variables, we require the following:\n",
    "    1. $X$ and $Y$ must only be able to take on a finite number of possible values\n",
    "        - This condition is satisfied since both $X$ and $Y$ can only take on two values (H or T)\n",
    "    2. $X$ and $Y$ must be numerical outcomes of a random phenomenon\n",
    "        - This condition is also satisfied, since a coin flip is definitely a random phenomenon\n",
    "\n",
    "- So **we've confirmed that $X$ and $Y$ are random variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the probability space for $X$ and $Y$?\n",
    "- **Recall**: a probability space requires three things\n",
    "    1. A **sample space** $\\Omega$ which is the set of all possible outcomes\n",
    "        - We already know there are only two possible outcomes so $\\Omega = \\{H,T\\}$\n",
    "    2. A **set of events** $F$ where each even is a set containing zero or more outcomes\n",
    "        - The set of possible events is the set of all possible combinations of these events\n",
    "            - This means $F = \\{H, T, \\text{Both }H\\text{ and }T,\\text{Neither }H\\text{ nor }T\\}$\n",
    "    3. The **assignment of probabilities** to the events\n",
    "        - We can figure out the probabilities for all the events in $F$ pretty easily\n",
    "            - $P(H)=1/2$\n",
    "            - $P(T)=1/2$\n",
    "            - $P(\\text{Both }H\\text{ and }T) = 0$ since we can't get both\n",
    "            - $P(\\text{Neither }H\\text{ nor }T) = 0$ since we have to get one of the two\n",
    "                \n",
    "- Therefore, **these two random variables are defined on the same probability space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are the outcomes of the two events related?\n",
    "\n",
    "- For example, if we get a heads on the first flip, what should we expect the second flip to be?\n",
    "    - Well, the gambler's fallacy tells us that just because we got a heads doesn't mean we're more likely to get a tails on the next flip\n",
    "- **In other words, if we know the value of $X$, we're not any better at predicting the value of $Y$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can write this relationship more mathematically as:\n",
    "\n",
    "$$\n",
    "P(Y=y | X=x) = P(Y=y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This equation is saying the same thing that we said above: the probability of the second flip being $y$ is not affected by the first flip being $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Independent Random Variables\n",
    "\n",
    "Two random variables $X$ and $Y$ defined on a common probability space $\\Omega$ are independent if, for any numbers $x$ and $y$ we have:\n",
    "\n",
    "$$\n",
    "P(X \\leq x \\cap Y \\leq y) = P(X \\leq x) \\cdot P(Y \\leq y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalized (and more confusing) version of this is for random variables $X_{1},X_{2},...,X_{n}$:\n",
    "\n",
    "$$\n",
    "P(X_{1}\\leq x_{1} \\cap X_{2}\\leq x_{2}\\cap ... \\cap X_{n}\\leq x_{n}) = \\prod_{i=1}^{n}P(X_{i}\\leq x_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Joint Distribution Function\n",
    "\n",
    "- If we look at the left side of the equation above, we can think of it as a function that takes in $n$ values, and returns a number between 0 and 1\n",
    "    - This is exactly what the **joint distribution function is**:\n",
    "    \n",
    "$$\n",
    "F(x_{1},x_{2},...,x_{n}) = P(X_{1}\\leq x_{1} \\cap X_{2}\\leq x_{2}\\cap ... \\cap X_{n}\\leq x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note**: the values are **cumulative** i.e. its the probability that each $X_{i}$ is **less than or equal to** $x_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Marginal Distribution Function\n",
    "\n",
    "- If we want to think about probabilities for a single random variable at a time, we use the **marginal distribution function**:\n",
    "\n",
    "$$\n",
    "F_{X_{i}}(x_{i}) = P(X_{i}\\leq x_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Ideas About Independence and Distribution Functions\n",
    "\n",
    "- We can see that it's easy to swap out the terms in our original definition for independence with the distribution functions:\n",
    "\n",
    "$$\n",
    "F(x_{1},x_{2},...,x_{n}) = F(x_{1})\\cdot F(x_{2})...F(x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Joint Mass Function\n",
    "\n",
    "- If we have **discrete** random variables $X_{1},X_{2},...,X_{n}$, then we define the **joint mass function** as:\n",
    "\n",
    "$$\n",
    "p(x_{1},x_{2},...,x_{n}) = P(X_{1}=x_{1} \\cap X_{2}=x_{2} \\cap...\\cap X_{n}=x_{n})\n",
    "$$\n",
    "\n",
    "- **Recall**: if $X_{i}$ was continuous instead of discrete, what is the value of $P(X_{1}=x_{1} \\cap X_{2}=x_{2} \\cap...\\cap X_{n}=x_{n})$\n",
    "    - **It's zero**\n",
    "        - Recall that a continuous random variable doesn't really ever equal a single value, but instead lies within a range\n",
    "            - E.g. What is the probability that it will rain 1cm tonight?\n",
    "                - ZERO\n",
    "                    - This is because the probability that 1cm falls (not 0.9999999 cm or 1.00000001) is negligible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, for our discrete random variables, they are independent if:\n",
    "\n",
    "$$\n",
    "p(x_{1},x_{2},...,x_{n}) = p_{X_{1}}(x_{1})\\cdot p_{X_{2}}(x_{2})\\cdot ... \\cdot p_{X_{n}}(x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Joint Density Function\n",
    "\n",
    "- Let's return to $F(x_{1},x_{2},...,x_{n}) = P(X_{1}\\leq x_{1} \\cap X_{2}\\leq x_{2}\\cap ... \\cap X_{n}\\leq x_{n})$ and we'll let each $X_{i}$ be a **continuous random variable**\n",
    "\n",
    "- If the random variables are all independent, then we have already shown $F(x_{1},x_{2},...,x_{n}) = F(x_{1})\\cdot F(x_{2})...F(x_{n})$\n",
    "\n",
    "- So now, what if we want to take the derivative of our **joint distribution function** with respect to one of the random variables, say $X_{1}$\n",
    "    - This means:\n",
    "    \n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{1}}F(x_{1},x_{2},...,x_{n}) = \\frac{\\partial}{\\partial x_{1}} \\left (F(x_{1})\\cdot F(x_{2})...F(x_{n}) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We recall from our first year calculus that if we're taking the partial derivative of the product of a bunch of functions where only one is a function of the variable, we can treat all the other functions as constants, i.e.:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}f(x)g(y)h(z) = g(y)h(z)\\frac{d}{d x}f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, this means that for our expression above:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{1}} \\left (F(x_{1})\\cdot F(x_{2})...F(x_{n}) \\right ) = \\frac{d}{dx_{1}}F_{X_{1}}(x_{1})\\left (  F(x_{2})...F(x_{n}) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall from earlier that the probability density function of a continuous random variable is defined as:\n",
    "\n",
    "$$\n",
    "f_{X}(x) = \\frac{d}{dx}F_{X}(x) = \\frac{d}{dx}\\left (P(X \\leq x) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plugging this into our partial derivative above:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{1}} \\left (F(x_{1})\\cdot F(x_{2})...F(x_{n}) \\right ) = f_{X_{1}}(x_{1})\\left (  F(x_{2})...F(x_{n}) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We could repeat this process for each random variable to give:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^{n}}{\\partial x_{1}\\partial x_{2}...\\partial x_{n}}F(x_{1},x_{2},...,x_{n}) = f_{X_{1}}(x_{1})\\cdot f_{X_{2}}(x_{2})\\cdot f_{X_{n}}(x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The expression $\\frac{\\partial^{n}}{\\partial x_{1}\\partial x_{2}...\\partial x_{n}}F(x_{1},x_{2},...,x_{n})$ is called the **joint density function** of the random variables and is denoted $f(x_{1},x_{2},...,x_{n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, as we derived above, if the random variables are all independent:\n",
    "\n",
    "$$\n",
    "f(x_{1},x_{2},...,x_{n}) = f_{X_{1}}(x_{1})\\cdot f_{X_{2}}(x_{2})\\cdot f_{X_{n}}(x_{n})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, if we swap out the $f$ for a $p$, this is basically the same as the discrete version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem 4.3\n",
    "\n",
    "If $X$ and $Y$ are independent and have finite second moments, they are uncorrelated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof of Theorem 4.3\n",
    "\n",
    "*Definitions*\n",
    "\n",
    "- Independence\n",
    "\n",
    "$$\n",
    "P(X \\leq x \\cap Y \\leq y) = P(X \\leq x) \\cdot P(Y \\leq y)\n",
    "$$\n",
    "\n",
    "- Moments\n",
    "\n",
    "$$\n",
    "n^{th}\\text{ Moment of the Mean} = E[X^{n}] \n",
    "$$\n",
    "\n",
    "- Uncorrelated\n",
    "\n",
    "$$\n",
    "Cov(X,Y) = 0\n",
    "$$\n",
    "\n",
    "- Covariance\n",
    "\n",
    "$$\n",
    "Cov(X,Y) = \\sigma_{XY} = E[XY] - E[X]E[Y]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, if $X$ and $Y$ are uncorrelated:\n",
    "\n",
    "$$\n",
    "E[XY] = E[X]E[Y]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll do the discrete case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E[X] = \\sum_{x}x\\cdot P(X=x); E[Y] = \\sum_{y}y\\cdot P(Y=y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\implies E[X]E[Y] = \\left (\\sum_{x}x\\cdot P(X=x) \\right ) \\cdot \\left (  \\sum_{y}y\\cdot P(Y=y) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\sum_{x}\\sum_{y}xy\\cdot P(X=x)P(Y=y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since $X$ and $Y$ are independent, we know $ P(X=x)P(Y=y) = P(X=x \\cap Y=y)$\n",
    "\n",
    "$$\n",
    "\\implies \\sum_{x}\\sum_{y}xy\\cdot P(X=x)P(Y=y) = \\sum_{x}\\sum_{y}xy\\cdot P(X=x \\cap Y=y) = E[XY]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In conclusion, we see that **independent $\\implies$ uncorrelated**\n",
    "    - *But does it go the other way?*\n",
    "        - We'll see (in the exercises) that **uncorrelated $\\nrightarrow$ independent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Expectation by Conditioning\n",
    "\n",
    "#### *Recall*: Partition Theorem\n",
    "\n",
    "- Let the events $B_{1},B_{2},...$ form a partition of $\\Omega$ i.e. they are disjoint (don't overlap) and their union contains all possible events in $\\Omega$\n",
    "\n",
    "- Then, for any event $A$:\n",
    "\n",
    "$$\n",
    "P(A) = \\sum_{i=1}^{\\infty}P(A\\cap B_{i})\n",
    "$$\n",
    "\n",
    "- *Cake Analogy*:This makes sense since a partition takes the \"cake\" of all possible events and \"slices\" it up\n",
    "    - You can't have the same bit of cake in two different slices\n",
    "    - If we think of $P(A)$ as the red icing on the cake, then the right side of the equation adds up the red icing on each individual slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Theorem for Expectation\n",
    "\n",
    "- The concept is the same as the one above, except we're calculating expected values instead of probabilities\n",
    "\n",
    "- Let $X$ and $Y$ be discrete, jointly distributed random variables\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_{y \\in Range(Y)}E[X|Y=y]\\cdot p_{Y}(y) = \\sum_{y \\in Range(Y)}E[X|Y=y]\\cdot P(Y=y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we think of this from a programming mindset, we loop through each possible $y$ value, and take the corresponding expected $x$ value, and multiply it by the probability of getting such an x value\n",
    "    - In the code below, let's assume `dict_expected_x` holds the $E[X|Y=y]$ value for each $y$, and `dict_prob_y` holds $p_{Y}(y)$\n",
    "\n",
    "```python\n",
    "E_X = 0\n",
    "for y in list_range_Y:\n",
    "    E_X += dict_expected_x[y] * dict_prob_y[y]    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So how do we even get the $E[X|Y=y]$ value?\n",
    "\n",
    "$$\n",
    "E[X|Y=y] = \\sum_{x\\in Range(X)}x\\cdot p_{(X|Y=y)}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And $p_{(X|Y=y)}(x)$ is defined as:\n",
    "\n",
    "$$\n",
    "p_{(X|Y=y)}(x) = P(X=x|Y=y) = \\frac{P(X=x \\cap Y=y)}{P(Y=y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can verify the expression above as follows:\n",
    "\n",
    "$$\n",
    "E[X|Y=y]p_{Y}(y) = \\sum_{y\\in Range(Y)}E[X|y=y]\\cdot P(Y=y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plugging in our expression for $E[X|Y=y]$ from above:\n",
    "$$\n",
    "= \\sum_{y\\in Range(Y)}\\left [ \\sum_{x\\in Range(X)}x\\cdot p_{(X|Y=y)}(x) \\right ]\\cdot P(Y=y) = \\sum_{y\\in Range(Y)}\\left [ \\sum_{x\\in Range(X)}x\\cdot \\frac{P(X=x \\cap Y=y)}{P(Y=y)} \\right ]\\cdot P(Y=y) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=  \\sum_{y\\in Range(Y)}\\sum_{x\\in Range(X)}x\\cdot P(X=x \\cap Y=y) =  \\sum_{x\\in Range(X)}\\sum_{y\\in Range(Y)}x\\cdot P(X=x \\cap Y=y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=  \\sum_{x\\in Range(X)}x \\left (\\sum_{y\\in Range(Y)}P(X=x \\cap Y=y) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, before we continue, let's think about what $\\sum_{y\\in Range(Y)}P(X=x \\cap Y=y)$ actually means\n",
    "    - It's saying that **for a fixed $x$ value**, we look through all possible $y$ values, and add up the probabilities\n",
    "        - But this just tells us the probability that $X=x$!\n",
    "- Therefore, $\\sum_{y\\in Range(Y)}P(X=x \\cap Y=y) = P(X=x)$\n",
    "\n",
    "$$\n",
    "\\implies \\sum_{x\\in Range(X)}x \\left (\\sum_{y\\in Range(Y)}P(X=x \\cap Y=y) \\right ) = \\sum_{x\\in Range(X)}x\\cdot P(X=x)\n",
    "$$\n",
    "\n",
    "- And this is just the definition of $E[X]$\n",
    "\n",
    "$$\n",
    "\\implies \\sum_{x\\in Range(X)}x = E[X]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Theorem for Expectation (w. Jointly Continuous Random Variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we're dealing with continuous random variables (instead of discrete), we can just swap out the sums for integrals:\n",
    "\n",
    "$$\n",
    "E[X] = \\int E[X|Y=y]f_{Y}(y)dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E[X|Y=y] = \\int x\\cdot f_{X|Y=y}(x)dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_{X|Y=y}(x)\\frac{f_{(X,Y)}(x,y)}{f_{Y}(y)}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
